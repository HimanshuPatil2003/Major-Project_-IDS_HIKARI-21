{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a48bacd2-12da-494c-a366-a5870beec48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Pre-Processing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9ad75d7-dff5-4824-b939-e1609d1baf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c66f6a28-eac5-405e-adce-df3a5d5bec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da1699ce-7a10-4ae9-8e08-81d34d8ba0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class FactoredRBM(nn.Module):\n",
    "    def __init__(self, n_visible, n_hidden, n_factors, learning_rate=0.01, batch_size=100, \n",
    "                 n_epochs=100, k=1, sigma=1.0):\n",
    "        \"\"\"\n",
    "        Initialize the Factored RBM\n",
    "        \n",
    "        Args:\n",
    "            n_visible (int): Number of visible units (features)\n",
    "            n_hidden (int): Number of hidden units\n",
    "            n_factors (int): Number of factors for the factored weights\n",
    "            learning_rate (float): Learning rate for optimization\n",
    "            batch_size (int): Size of mini-batches\n",
    "            n_epochs (int): Number of training epochs\n",
    "            k (int): Number of Gibbs sampling steps\n",
    "            sigma (float): Standard deviation for Gaussian visible units\n",
    "        \"\"\"\n",
    "        super(FactoredRBM, self).__init__()\n",
    "        \n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_factors = n_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.k = k\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        # Initialize factored weights with smaller values\n",
    "        self.W_v = nn.Parameter(torch.randn(n_visible, n_factors) * 0.001)\n",
    "        self.W_h = nn.Parameter(torch.randn(n_factors, n_hidden) * 0.001)\n",
    "        \n",
    "        # Initialize biases\n",
    "        self.visible_bias = nn.Parameter(torch.zeros(n_visible))\n",
    "        self.hidden_bias = nn.Parameter(torch.zeros(n_hidden))\n",
    "        \n",
    "        # For tracking feature importance\n",
    "        self.feature_importance = None\n",
    "        \n",
    "        # Initialize scaler\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    def free_energy(self, v):\n",
    "        \"\"\"Calculate the free energy of the visible units\"\"\"\n",
    "        v_term = -torch.sum((v - self.visible_bias).pow(2) / (2 * self.sigma**2), dim=1)\n",
    "        wx = torch.matmul(torch.matmul(v, self.W_v), self.W_h)\n",
    "        # Add numerical stability\n",
    "        hidden_term = torch.sum(torch.log1p(torch.exp(torch.clamp(wx + self.hidden_bias, -20, 20))), dim=1)\n",
    "        return -hidden_term - v_term\n",
    "    \n",
    "    def sample_hidden(self, v):\n",
    "        \"\"\"Sample hidden units given visible units\"\"\"\n",
    "        wx = torch.matmul(torch.matmul(v, self.W_v), self.W_h)\n",
    "        activation = wx + self.hidden_bias\n",
    "        # Clamp values for numerical stability\n",
    "        activation = torch.clamp(activation, -20, 20)\n",
    "        p_h = torch.sigmoid(activation)\n",
    "        # Ensure probabilities are valid\n",
    "        p_h = torch.clamp(p_h, 0, 1)\n",
    "        return p_h, torch.bernoulli(p_h)\n",
    "    \n",
    "    def sample_visible(self, h):\n",
    "        \"\"\"Sample visible units given hidden units\"\"\"\n",
    "        wx = torch.matmul(torch.matmul(h, self.W_h.t()), self.W_v.t())\n",
    "        mean_v = wx + self.visible_bias\n",
    "        # For continuous data, sample from Gaussian with learned mean\n",
    "        sample_v = mean_v + torch.randn_like(mean_v) * self.sigma\n",
    "        return mean_v, sample_v\n",
    "    \n",
    "    def gibbs_step(self, v):\n",
    "        \"\"\"Perform one step of Gibbs sampling\"\"\"\n",
    "        p_h, h = self.sample_hidden(v)\n",
    "        mean_v, v = self.sample_visible(h)\n",
    "        return mean_v, v, p_h, h\n",
    "    \n",
    "    def contrastive_divergence(self, v_pos):\n",
    "        \"\"\"Perform k steps of contrastive divergence\"\"\"\n",
    "        # Positive phase\n",
    "        p_h_pos, h_pos = self.sample_hidden(v_pos)\n",
    "        \n",
    "        # Negative phase\n",
    "        v_neg = v_pos.clone()\n",
    "        for _ in range(self.k):\n",
    "            _, v_neg, _, _ = self.gibbs_step(v_neg)\n",
    "        \n",
    "        p_h_neg, _ = self.sample_hidden(v_neg)\n",
    "        \n",
    "        return v_pos, v_neg, p_h_pos, p_h_neg\n",
    "    \n",
    "    def calculate_feature_importance(self):\n",
    "        \"\"\"Calculate feature importance scores\"\"\"\n",
    "        W = torch.matmul(self.W_v, self.W_h)\n",
    "        importance = torch.sum(W.pow(2), dim=1)\n",
    "        self.feature_importance = importance.detach().numpy()\n",
    "        return self.feature_importance\n",
    "    \n",
    "    def fit(self, data, y=None, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the Factored RBM\n",
    "        \n",
    "        Args:\n",
    "            data: numpy array or torch tensor of training data\n",
    "            y: labels (not used, included for sklearn compatibility)\n",
    "            verbose: whether to print progress\n",
    "        \"\"\"\n",
    "        # Scale the data\n",
    "        if isinstance(data, np.ndarray):\n",
    "            data = self.scaler.fit_transform(data)\n",
    "            data = torch.FloatTensor(data)\n",
    "        \n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        n_batches = len(data) // self.batch_size\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            epoch_error = 0\n",
    "            \n",
    "            for i in range(n_batches):\n",
    "                batch_start = i * self.batch_size\n",
    "                batch_end = (i + 1) * self.batch_size\n",
    "                v_pos = data[batch_start:batch_end]\n",
    "                \n",
    "                # Contrastive Divergence\n",
    "                v_pos, v_neg, p_h_pos, p_h_neg = self.contrastive_divergence(v_pos)\n",
    "                \n",
    "                # Calculate gradients using free energy\n",
    "                cost = torch.mean(self.free_energy(v_pos) - self.free_energy(v_neg))\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                cost.backward()\n",
    "                \n",
    "                # Clip gradients for stability\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_error += cost.item()\n",
    "            \n",
    "            if verbose and (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch {epoch + 1}/{self.n_epochs}, Error: {epoch_error / n_batches:.4f}')\n",
    "        \n",
    "        # Calculate final feature importance\n",
    "        self.calculate_feature_importance()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Transform data using the hidden layer representation\n",
    "        \n",
    "        Args:\n",
    "            X: input data\n",
    "            y: labels (not used, included for sklearn compatibility)\n",
    "        \"\"\"\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = self.scaler.transform(X)  # Use transform, not fit_transform\n",
    "            X = torch.FloatTensor(X)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            hidden_probs, _ = self.sample_hidden(X)\n",
    "        return hidden_probs.numpy()\n",
    "    \n",
    "    def get_feature_ranking(self):\n",
    "        \"\"\"Get features ranked by importance\"\"\"\n",
    "        if self.feature_importance is None:\n",
    "            self.calculate_feature_importance()\n",
    "        return np.argsort(-self.feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e86ad90e-ab9b-47cd-8184-323693b0e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_f1 = dict()\n",
    "KNN_f1 = dict()\n",
    "RFC_f1 = dict()\n",
    "LGBM_f1 = dict()\n",
    "XGB_f1 = dict()\n",
    "CB_f1 = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "070c2ca1-9c22-4206-bae9-fb92f5a20f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of feature 71\n",
      "Epoch 10/100, Error: -2895165666606156.0000\n",
      "Epoch 20/100, Error: -51378029036553408.0000\n",
      "Epoch 30/100, Error: -268944151074643904.0000\n",
      "Epoch 40/100, Error: -864054554953229056.0000\n",
      "Epoch 50/100, Error: -2130279326040873216.0000\n",
      "Epoch 60/100, Error: -4446230531840812544.0000\n",
      "Epoch 70/100, Error: -8275814465349258240.0000\n",
      "Epoch 80/100, Error: -14167887403639595008.0000\n",
      "Epoch 90/100, Error: -22755235998812303360.0000\n",
      "Epoch 100/100, Error: -34754990589867966464.0000\n",
      "SVM Model:-\n",
      "[LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gauta\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal     0.0000    0.0000    0.0000    103560\n",
      "      Attack     0.0675    1.0000    0.1265      7496\n",
      "\n",
      "    accuracy                         0.0675    111056\n",
      "   macro avg     0.0337    0.5000    0.0632    111056\n",
      "weighted avg     0.0046    0.0675    0.0085    111056\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[     0 103560]\n",
      " [     0   7496]]\n",
      "KNN model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gauta\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gauta\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gauta\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal     0.9325    1.0000    0.9651    103560\n",
      "      Attack     0.0000    0.0000    0.0000      7496\n",
      "\n",
      "    accuracy                         0.9325    111056\n",
      "   macro avg     0.4663    0.5000    0.4825    111056\n",
      "weighted avg     0.8696    0.9325    0.8999    111056\n",
      "\n",
      "Confusion Matrix:\n",
      "[[103560      0]\n",
      " [  7496      0]]\n",
      "Random Forest:-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gauta\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gauta\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gauta\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal     0.9908    0.6796    0.8063    103560\n",
      "      Attack     0.1710    0.9132    0.2881      7496\n",
      "\n",
      "    accuracy                         0.6954    111056\n",
      "   macro avg     0.5809    0.7964    0.5472    111056\n",
      "weighted avg     0.9355    0.6954    0.7713    111056\n",
      "\n",
      "Confusion Matrix:\n",
      "[[70384 33176]\n",
      " [  651  6845]]\n",
      "LGBM Model:-\n",
      "[LightGBM] [Info] Number of positive: 83069, number of negative: 414022\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 213\n",
      "[LightGBM] [Info] Number of data points in the train set: 497091, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.167110 -> initscore=-1.606248\n",
      "[LightGBM] [Info] Start training from score -1.606248\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gauta\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gauta\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gauta\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal     0.9325    1.0000    0.9651    103560\n",
      "      Attack     0.0000    0.0000    0.0000      7496\n",
      "\n",
      "    accuracy                         0.9325    111056\n",
      "   macro avg     0.4663    0.5000    0.4825    111056\n",
      "weighted avg     0.8696    0.9325    0.8999    111056\n",
      "\n",
      "Confusion Matrix:\n",
      "[[103560      0]\n",
      " [  7496      0]]\n",
      "XGB Model:-\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal     0.9325    1.0000    0.9651    103560\n",
      "      Attack     0.0000    0.0000    0.0000      7496\n",
      "\n",
      "    accuracy                         0.9325    111056\n",
      "   macro avg     0.4663    0.5000    0.4825    111056\n",
      "weighted avg     0.8696    0.9325    0.8999    111056\n",
      "\n",
      "Confusion Matrix:\n",
      "[[103560      0]\n",
      " [  7496      0]]\n",
      "CatBoost Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gauta\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gauta\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gauta\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5952799\ttotal: 210ms\tremaining: 3m 29s\n",
      "100:\tlearn: 0.3377252\ttotal: 7.29s\tremaining: 1m 4s\n",
      "200:\tlearn: 0.3377252\ttotal: 13.9s\tremaining: 55.4s\n",
      "300:\tlearn: 0.3377252\ttotal: 18s\tremaining: 41.8s\n",
      "400:\tlearn: 0.3377252\ttotal: 23.1s\tremaining: 34.5s\n",
      "500:\tlearn: 0.3377252\ttotal: 26.9s\tremaining: 26.8s\n",
      "600:\tlearn: 0.3377252\ttotal: 32.6s\tremaining: 21.7s\n",
      "700:\tlearn: 0.3377252\ttotal: 37s\tremaining: 15.8s\n",
      "800:\tlearn: 0.3377252\ttotal: 42.6s\tremaining: 10.6s\n",
      "900:\tlearn: 0.3377252\ttotal: 47.3s\tremaining: 5.2s\n",
      "999:\tlearn: 0.3377252\ttotal: 51.5s\tremaining: 0us\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gauta\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gauta\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gauta\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal     0.9325    1.0000    0.9651    103560\n",
      "      Attack     0.0000    0.0000    0.0000      7496\n",
      "\n",
      "    accuracy                         0.9325    111056\n",
      "   macro avg     0.4663    0.5000    0.4825    111056\n",
      "weighted avg     0.8696    0.9325    0.8999    111056\n",
      "\n",
      "Confusion Matrix:\n",
      "[[103560      0]\n",
      " [  7496      0]]\n",
      "No of feature 61\n",
      "Epoch 10/100, Error: -919940623020472.1250\n",
      "Epoch 20/100, Error: -16101442659917486.0000\n",
      "Epoch 30/100, Error: -83975499018375680.0000\n",
      "Epoch 40/100, Error: -269381771752632096.0000\n",
      "Epoch 50/100, Error: -663586898483613952.0000\n",
      "Epoch 60/100, Error: -1384260944434877440.0000\n",
      "Epoch 70/100, Error: -2575415889547164160.0000\n",
      "Epoch 80/100, Error: -4407654697962359808.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 13\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo of feature\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_feature)\n\u001b[0;32m      4\u001b[0m selector \u001b[38;5;241m=\u001b[39m FactoredRBM(\n\u001b[0;32m      5\u001b[0m     n_visible\u001b[38;5;241m=\u001b[39mX_train_resampled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m      6\u001b[0m     n_hidden\u001b[38;5;241m=\u001b[39mnum_feature,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[0;32m     12\u001b[0m     )\n\u001b[1;32m---> 13\u001b[0m selector\u001b[38;5;241m.\u001b[39mfit(X_train_resampled, y_train_resampled)\n\u001b[0;32m     14\u001b[0m X_train_selected \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mtransform(X_train_resampled, y_train_resampled)\n\u001b[0;32m     15\u001b[0m X_test_selected \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mtransform(X_test_transformed)\n",
      "Cell \u001b[1;32mIn[11], line 129\u001b[0m, in \u001b[0;36mFactoredRBM.fit\u001b[1;34m(self, data, y, verbose)\u001b[0m\n\u001b[0;32m    126\u001b[0m v_pos \u001b[38;5;241m=\u001b[39m data[batch_start:batch_end]\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Contrastive Divergence\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m v_pos, v_neg, p_h_pos, p_h_neg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrastive_divergence(v_pos)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# Calculate gradients using free energy\u001b[39;00m\n\u001b[0;32m    132\u001b[0m cost \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfree_energy(v_pos) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfree_energy(v_neg))\n",
      "Cell \u001b[1;32mIn[11], line 91\u001b[0m, in \u001b[0;36mFactoredRBM.contrastive_divergence\u001b[1;34m(self, v_pos)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk):\n\u001b[0;32m     89\u001b[0m     _, v_neg, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgibbs_step(v_neg)\n\u001b[1;32m---> 91\u001b[0m p_h_neg, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_hidden(v_neg)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m v_pos, v_neg, p_h_pos, p_h_neg\n",
      "Cell \u001b[1;32mIn[11], line 64\u001b[0m, in \u001b[0;36mFactoredRBM.sample_hidden\u001b[1;34m(self, v)\u001b[0m\n\u001b[0;32m     62\u001b[0m p_h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(activation)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Ensure probabilities are valid\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m p_h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(p_h, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m p_h, torch\u001b[38;5;241m.\u001b[39mbernoulli(p_h)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for x in range(71, 10, -10):\n",
    "    num_feature = x\n",
    "    print(\"No of feature\", num_feature)\n",
    "    selector = FactoredRBM(\n",
    "        n_visible=X_train_resampled.shape[1],\n",
    "        n_hidden=num_feature,\n",
    "        n_factors=9,\n",
    "        learning_rate=0.1,\n",
    "        batch_size=4096,\n",
    "        n_epochs=100,\n",
    "        k=3\n",
    "    )\n",
    "    selector.fit(X_train_resampled, y_train_resampled)\n",
    "    X_train_selected = selector.transform(X_train_resampled, y_train_resampled)\n",
    "    X_test_selected = selector.transform(X_test_transformed)\n",
    "    SVM_f1[x] = SVM_selector(X_train_selected, y_train_resampled, X_test_selected, y_test)\n",
    "    KNN_f1[x] = KNN_Classifier(X_train_selected, y_train_resampled, X_test_selected, y_test)\n",
    "    RFC_f1[x] = RandomForest_Classifier(X_train_selected, y_train_resampled, X_test_selected, y_test)\n",
    "    LGBM_f1[x] = LGBM_Classifier(X_train_selected, y_train_resampled, X_test_selected, y_test)\n",
    "    XGB_f1[x] = XGB_Classifier(X_train_selected, y_train_resampled, X_test_selected, y_test)\n",
    "    CB_f1[x] = CatBoost_Classifier(X_train_selected, y_train_resampled, X_test_selected, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1738ce47-4ccc-48d2-a1cc-7bbb0d3711e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_plot(dictionary):\n",
    "    keys = list(dictionary.keys())\n",
    "    values = list(dictionary.values())\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(keys, values, marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel('No of Features')\n",
    "    plt.ylabel('F1-Score')\n",
    "    plt.title('No of Features vs F1-Score')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f680f1-4727-4b9f-86ad-b94de42328ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_plot(SVM_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a5c2a5-9d7f-48e9-a237-2e79a0ea55d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_plot(KNN_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babe947b-00fd-4938-82c6-486e5812aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_plot(RFC_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe68c806-7fb3-4a69-a6fe-29c27d106456",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_plot(LGBM_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f0ea7e-bba4-41b7-afd0-072617ab63d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_plot(XGB_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbee0fb-7eaf-4dd8-a462-bc8d4f1dee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_plot(CB_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75679654-c4f5-4255-a9ed-082872e6938e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
